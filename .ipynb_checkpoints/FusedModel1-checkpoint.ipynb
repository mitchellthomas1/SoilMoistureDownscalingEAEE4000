{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b941c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from scipy.stats import pearsonr\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperparameteroptimization import tune_1st_NN, tune_2nd_NN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from utils import * \n",
    "from buildmodel import prepare_10kmdata, prepare_40mdata, get_landcover_bands,z_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873b83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define file path and files\n",
    "#file1: satellite data for randomly sampled points across U.S.\n",
    "#file2: satellite export data associated with in situ points\n",
    "\n",
    "path = '/Users/Mitchell/Documents/MLEnvironment/SoilMoistureDownscalingEAEE4000/'\n",
    "file1 =  path + 'DataDownload/pointsamples/Point100Sample.csv'\n",
    "# data file\n",
    "file2 = path + 'DataDownload/InSitu/InSituGEEOutputs/SMGaugePoints2.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf29e9be",
   "metadata": {},
   "source": [
    "### Step 1: Build first neural network ###\n",
    "This neural network will be trained on ______ random points from contiguous USA. Data has been downloaded and reprojected to 10 km EPSG:4326 resolution in Google Earth Engine. Data for this section is from \"file1\".\n",
    "\n",
    "Predictors: \n",
    "* All 13 bands plus NDVI from Sentinel-2\n",
    "* VV, VH, and angle measurement from Sentinel-1\n",
    "* mTPI ('elevation')\n",
    "* landcover classification (categorical data transformed with One Hot Encoding)\n",
    "\n",
    "Predictand:\n",
    "* Surface Soil Moisture as measured by SMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd4da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the landcover bands that will be used in this study. This is needed for future one hot encoding.\n",
    "lc_bands = list(get_landcover_bands([file1,file2]))\n",
    "# define predictors and predictand in file\n",
    "predictors = ['B1', 'B10', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8',\n",
    "       'B8A', 'B9', 'VH', 'VV', 'angle', 'elevation', 'NDVI']\n",
    "predictand = ['ssm']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b1ef96",
   "metadata": {},
   "source": [
    "##### Import Data #####\n",
    "\n",
    "Below data for this first neural network is imported, from the function prepare_10kmdata stored in the helper file buildmodel. This file imports, normalizes, and concatenates data time series from all random points.\n",
    "\n",
    "We also import data for the second neural network at this point so we can evaluate how the first neural network (that predicts SMAP at 10km from remote sensing and land/soil/elevation data) correlates to in situ measurements when the test data at 40m is passed through  to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c4727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in data for part I (10km resolution)\n",
    "X_train, y_train, X_test, y_test, X_length = prepare_10kmdata(file1, predictors, predictand, lc_bands, dropna = True)\n",
    "\n",
    "# Bring in data for part II (30m resolution) (in order to conduct preliminary evaluation of first neural net)\n",
    "insitu_file = path+'DataDownload/InSitu/SoilMoistureDataFrameGreaterThan80.csv'\n",
    "predictors1 = ['B1', 'B10', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8',\n",
    "       'B8A', 'B9', 'VH', 'VV', 'angle', 'elevation', 'NDVI']\n",
    "predictors2 = ['ssm']\n",
    "predictand = ['InSituSM']\n",
    "X2_1_train, X2_2_train, y2_train, X2_1_test, X2_2_test, y2_test, X2_length = prepare_40mdata(\n",
    "                            file2,insitu_file, predictors1, predictors2, predictand, lc_bands, dropna = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6aaa7",
   "metadata": {},
   "source": [
    "##### Build First Neural Network\n",
    "\n",
    "Now we build the first neural network. We use the keras functional Model API as opposed to to the sequential API as demonstrated in class to add additional flexibility that will enable our unique model design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac04e1",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning**\n",
    "To tune hyperparameters, set tune_params to True. However this has already been run and the results have been implemented into the hyperparameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d246cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/Mitchell/opt/anaconda3/envs/keras/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3457\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"/var/folders/fx/kh_2yrm96pbg2rdsgbbj16mr0000gn/T/ipykernel_34415/3069970137.py\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    tune_1st_NN(max_evals = 5)\n",
      "  File \u001b[1;32m\"/Users/Mitchell/Documents/MLEnvironment/SoilMoistureDownscalingEAEE4000/hyperparameteroptimization.py\"\u001b[0m, line \u001b[1;32m92\u001b[0m, in \u001b[1;35mtune_1st_NN\u001b[0m\n    best_run, best_model = optim.minimize(model=model,\n",
      "  File \u001b[1;32m\"/Users/Mitchell/opt/anaconda3/envs/keras/lib/python3.10/site-packages/hyperas/optim.py\"\u001b[0m, line \u001b[1;32m59\u001b[0m, in \u001b[1;35mminimize\u001b[0m\n    best_run, space = base_minimizer(model=model,\n",
      "  File \u001b[1;32m\"/Users/Mitchell/opt/anaconda3/envs/keras/lib/python3.10/site-packages/hyperas/optim.py\"\u001b[0m, line \u001b[1;32m98\u001b[0m, in \u001b[1;35mbase_minimizer\u001b[0m\n    model_str = get_hyperopt_model_string(model, data, functions, notebook_name, verbose, stack)\n",
      "  File \u001b[1;32m\"/Users/Mitchell/opt/anaconda3/envs/keras/lib/python3.10/site-packages/hyperas/optim.py\"\u001b[0m, line \u001b[1;32m175\u001b[0m, in \u001b[1;35mget_hyperopt_model_string\u001b[0m\n    model_string = remove_imports(model_string)\n",
      "  File \u001b[1;32m\"/Users/Mitchell/opt/anaconda3/envs/keras/lib/python3.10/site-packages/hyperas/utils.py\"\u001b[0m, line \u001b[1;32m60\u001b[0m, in \u001b[1;35mremove_imports\u001b[0m\n    tree = ast.parse(source)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/Mitchell/opt/anaconda3/envs/keras/lib/python3.10/ast.py\"\u001b[0;36m, line \u001b[0;32m50\u001b[0;36m, in \u001b[0;35mparse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def model(X_train, Y_train,X_test, Y_test):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "tune_params = True\n",
    "tune_1st_NN(max_evals = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data():\n",
    "#     path = '/Users/Mitchell/Documents/MLEnvironment/SoilMoistureDownscalingEAEE4000/'\n",
    "#     file1 =  path + 'DataDownload/pointsamples/Point100Sample.csv'\n",
    "#     file2 = path + 'DataDownload/InSitu/InSituGEEOutputs/SMGaugePoints2.csv'\n",
    "#     lc_bands = list(get_landcover_bands([file1,file2]))\n",
    "#     predictors = ['B1', 'B10', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8',\n",
    "#            'B8A', 'B9', 'VH', 'VV', 'angle', 'elevation', 'NDVI']\n",
    "#     predictand = ['ssm']\n",
    "#     X_t, y_t, _, _, _ = prepare_10kmdata(file1,\n",
    "#                      predictors, predictand, lc_bands, dropna = True)\n",
    "#     index = int(0.80*len(X_t))\n",
    "#     X_train = X_t[:index] \n",
    "#     Y_train = y_t[:index]\n",
    "#     X_test = X_t[index : ]\n",
    "#     Y_test = y_t[index :]\n",
    "#     return X_train, Y_train,X_test, Y_test \n",
    "\n",
    "# def model(X_train, Y_train,X_test, Y_test):\n",
    "#     # --- Define layers with hyperparameter tuning built in -----\n",
    "\n",
    "#     from tensorflow.keras.models import Model, load_model\n",
    "#     from tensorflow.keras.layers import Dense, Dropout\n",
    "#     from tensorflow.keras import Input\n",
    "\n",
    "#     # Hyperparameter Tuning options\n",
    "#     n_neuron_choice = {{choice([16, 32, 64, 128])}}\n",
    "#     learning_rate_choice = {{choice([0.01,0.005, 0.001,0.0005, 0.0001,0.00001])}}\n",
    "#     dropout_choice = {{uniform(0.,1.)}}\n",
    "#     epoch_choice = {{choice([5,10,15,20,30,40,50])}}\n",
    "#     layers_choice = {{choice(['two', 'three'])}}\n",
    "#     activation     = 'relu'\n",
    "#     minibatch_size = 64\n",
    "#     # Model input\n",
    "#     input1 = Input(shape= (X_train.shape[1],))\n",
    "#     # Layer 1\n",
    "#     dense1 = Dense(n_neuron_choice,  activation=activation)\n",
    "#     # Layer 2\n",
    "#     dense2 = Dense(n_neuron_choice,  activation=activation)\n",
    "#     #Layer 3\n",
    "#     dense3 = Dense(n_neuron_choice,  activation=activation)\n",
    "#     # Dropout \n",
    "#     dropout_layer = Dropout(dropout_choice)\n",
    "#     #Output\n",
    "#     output_layer_1 = Dense(Y_train.shape[1],  activation='linear')\n",
    "#     # --- Build Model ---\n",
    "#     x = dense1(input1)\n",
    "#     x = dense2(x)\n",
    "#     x = dense3(x)\n",
    "#     x = dropout_layer(x)\n",
    "#     output1 = output_layer_1(x)\n",
    "#     model = Model(inputs = input1, outputs = output1)\n",
    "#     model.compile(loss='mse',\n",
    "#                 optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_choice))\n",
    "#     early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "#     model.fit(X_train, Y_train, \n",
    "#                         batch_size      = minibatch_size,\n",
    "#                         epochs          = epoch_choice,\n",
    "#                         validation_data=(X_test, Y_test),\n",
    "#                         verbose         = 2,\n",
    "#                         callbacks       = [early_stop])\n",
    "#     loss = model.evaluate(X_test,  Y_test, verbose=0)\n",
    "#     return {'loss': loss, 'status': STATUS_OK, 'model': model}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d633d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_run, best_model = optim.minimize(model=model,\n",
    "#                                           data= data,\n",
    "#                                           algo= tpe.suggest,\n",
    "#                                           max_evals=100,\n",
    "#                                           trials= Trials(),\n",
    "#                                           notebook_name = 'FusedModel1',\n",
    "#                                           eval_space = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cdd142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters based on tuning\n",
    "n_neuron       = 64\n",
    "activation     = 'relu'\n",
    "num_epochs     = 15\n",
    "learning_rate  = 0.0005\n",
    "minibatch_size = 64\n",
    "model_num      = 1\n",
    "dropout_rate = 0\n",
    "n_layers = 'three'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cf4189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define layers -----\n",
    "# Model input\n",
    "input1 = Input(shape= (X_train.shape[1],))\n",
    "# Layer 1\n",
    "dense1 = Dense(n_neuron,  activation=activation)\n",
    "# Layer 2\n",
    "dense2 = Dense(n_neuron,  activation=activation)\n",
    "#Layer 3\n",
    "dense3 = Dense(n_neuron,  activation=activation)\n",
    "## Dropout (have been playing around with this)\n",
    "dropout_layer = Dropout(rate = dropout_rate)\n",
    "#Output\n",
    "output_layer_1 = Dense(y_train.shape[1],  activation='linear')\n",
    "\n",
    "# --- Build Model ---\n",
    "x = dense1(input1)\n",
    "x = dense2(x)\n",
    "if n_layers == 3:\n",
    "    x = dense3(x)\n",
    "x = dropout_layer(x)\n",
    "output1 = output_layer_1(x)\n",
    "\n",
    "model1 = Model(inputs = input1, outputs = output1)\n",
    "model1.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64de071",
   "metadata": {},
   "source": [
    "##### Train Model\n",
    "We train the model on 10km input data and ______ points to predict SMAP at 10 km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "history1 = model1.fit(X_train, y_train, \n",
    "            batch_size      = minibatch_size,\n",
    "            epochs          = num_epochs,\n",
    "            validation_split =0.2,\n",
    "            verbose         = 2,\n",
    "            callbacks       = [early_stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6647e",
   "metadata": {},
   "source": [
    "#### Evaluate training of first model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b9f30",
   "metadata": {},
   "source": [
    "We first evaluate how this model does at actually predicting its predictand: SMAP surface soil moisture at 10km resolution, and plot the history of the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c81d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model1.evaluate(X_test, y_test)\n",
    "plot_history(history1)\n",
    "plt.title('First NN Training, Loss = {}'.format(loss))\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23211371",
   "metadata": {},
   "source": [
    "Now we evaluate if this can better predict in situ soil moisture at 40m when 40m satellite data is passed to the same trained model.\n",
    "\n",
    "This rests under the assumptions that:\n",
    "1. the relationships between soil mositure and our non-soil mositure satellite predictands holds up similarly at a high resolution vs the 10km resolution on which it was trained.\n",
    "2. ____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87262e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how this first testing does on its own for predicting in situ! We will plot this later when we compare results\n",
    "# But since the model weights are changing with later training we get the accuracy values now\n",
    "\n",
    "firstnn_y_test_pre = model1.predict(X2_1_test)[:,0]\n",
    "in_situ_truth = y2_test[:,0]\n",
    "firstnn_loss = metrics.mean_squared_error(in_situ_truth, firstnn_y_test_pre)\n",
    "firstnn_pearson_r = pearsonr(in_situ_truth, firstnn_y_test_pre).statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5244d49c",
   "metadata": {},
   "source": [
    "Now we build the second neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3584f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if tune_params:\n",
    "#     def data():\n",
    "#         path = '/Users/Mitchell/Documents/MLEnvironment/SoilMoistureDownscalingEAEE4000/'\n",
    "#         file1 =  path + 'DataDownload/pointsamples/Point100Sample.csv'\n",
    "#         file2 = path + 'DataDownload/InSitu/InSituGEEOutputs/SMGaugePoints2.csv'\n",
    "#         insitu_file = path+'DataDownload/InSitu/SoilMoistureDataFrameGreaterThan80.csv'\n",
    "#         lc_bands = list(get_landcover_bands([file1,file2]))\n",
    "#         predictors = ['B1', 'B10', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8',\n",
    "#                'B8A', 'B9', 'VH', 'VV', 'angle', 'elevation', 'NDVI']\n",
    "#         predictand = ['ssm']\n",
    "#         X1_tr, X2_tr, y2_tr, _,_,_, _ = prepare_40mdata(\n",
    "#                             file2,insitu_file, predictors1, predictors2, predictand, lc_bands, dropna = True)\n",
    "#         index = int(0.80*len(y2_tr))\n",
    "#         X1_train = X1_tr[:index] \n",
    "#         X2_train = X2_tr[:index] \n",
    "#         Y_train = y2_tr[:index]\n",
    "#         X1_test = X1_tr[index:] \n",
    "#         X2_test = X2_tr[index:] \n",
    "#         Y_test = y2_tr[index:]\n",
    "        \n",
    "#         return X1_train,X2_train, Y_train, X1_test,X2_test, Y_test\n",
    "\n",
    "#     def model(X1_train,X2_train, Y_train, X1_test,X2_test, Y_test):\n",
    "#         # hyperparameters from last tuning:\n",
    "#         # set hyperparameters based on tuning\n",
    "#         n_neuron       = 64\n",
    "#         activation     = 'relu'\n",
    "#         num_epochs     = 15\n",
    "#         learning_rate  = 0.0005\n",
    "#         minibatch_size = 64\n",
    "#         model_num      = 1\n",
    "#         dropout_rate = 0\n",
    "#         n_layers = 'three'\n",
    "#         # Model input\n",
    "#         input1 = Input(shape= (X_train.shape[1],))\n",
    "#         dense1 = Dense(n_neuron,  activation=activation)\n",
    "#         dense2 = Dense(n_neuron,  activation=activation)\n",
    "#         dense3 = Dense(n_neuron,  activation=activation)\n",
    "#         dropout_layer = Dropout(rate = dropout_rate)\n",
    "#         output_layer_1 = Dense(y_train.shape[1],  activation='linear')\n",
    "#         x = dense1(input1)\n",
    "#         x = dense2(x)\n",
    "#         if n_layers == 3:\n",
    "#             x = dense3(x)\n",
    "#         x = dropout_layer(x)\n",
    "#         output1 = output_layer_1(x)\n",
    "\n",
    "#         model1 = Model(inputs = input1, outputs = output1)\n",
    "#         model1.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "#         early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "#         model.fit(X_train, Y_train, \n",
    "#                         batch_size      = minibatch_size,\n",
    "#                         epochs          = epoch_choice,\n",
    "#                         validation_data=(X_test, Y_test),\n",
    "#                         verbose         = 2,\n",
    "#                         callbacks       = [early_stop])\n",
    "#         # --- Define layers with hyperparameter tuning built in -----\n",
    "\n",
    "#         from tensorflow.keras.models import Model, load_model\n",
    "#         from tensorflow.keras.layers import Dense, Dropout\n",
    "#         from tensorflow.keras import Input\n",
    "\n",
    "#         # Hyperparameter Tuning options\n",
    "#         n_neuron_choice = {{choice([16, 32, 64, 128])}}\n",
    "#         learning_rate_choice = {{choice([0.01,0.005, 0.001,0.0005, 0.0001,0.00001])}}\n",
    "#         dropout_choice = {{uniform(0.,1.)}}\n",
    "#         epoch_choice = {{choice([5,10,15,20,30,40,50])}}\n",
    "#         layers_choice = {{choice(['two', 'three'])}}\n",
    "#         trainable =  = {{choice(['none','last', 'last2', 'last3'])}}\n",
    "#         activation     = 'relu'\n",
    "#         minibatch_size = 64\n",
    "\n",
    "\n",
    "#         input2 = Input(shape= (1,))\n",
    "\n",
    "#         input_concat = Concatenate()([output1, input2 ])\n",
    "#         dense1_2 = Dense(n_neuron_choice,  activation=activation)\n",
    "#         # Layer 2\n",
    "#         dense2_2 = Dense(n_neuron_choice,  activation=activation)\n",
    "#         # Layer 3\n",
    "#         dense3_2 = Dense(n_neuron_choice,  activation=activation)\n",
    "#         #output\n",
    "#         output_layer_2 = Dense(y_train.shape[1],  activation='linear')\n",
    "#         #dropout\n",
    "#         dropout_layer = Dropout(rate = dropout_choice)\n",
    "        \n",
    "#         dense1.Trainable = False\n",
    "#         dense2.Trainable = False\n",
    "#         dense3.Trainable = False\n",
    "#         if trainable == 'last':\n",
    "#             dense3.Trainable = True\n",
    "#         elif trainable == 'last2':\n",
    "#             dense2.Trainable = True\n",
    "#             dense3.Trainable = True\n",
    "#         elif trainable == 'last3':\n",
    "#             dense1.Trainable = True\n",
    "#             dense2.Trainable = True\n",
    "#             dense3.Trainable = True\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "#         x = dense1_2(input_concat)\n",
    "#         x = dense2_2(x)\n",
    "#         if layers_choice == 'three':\n",
    "#             x = dense3_2(x)\n",
    "#         x = dropout_layer(x)\n",
    "#         output2 = output_layer_2(x)\n",
    "\n",
    "#         model2 = Model(inputs = [input1, input2], outputs = output2)\n",
    "        \n",
    "#         model.compile(loss='mse',\n",
    "#                     optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_choice))\n",
    "\n",
    "#         early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "#         model.fit((X1_train,X2_train), Y_train, \n",
    "#                             batch_size      = minibatch_size,\n",
    "#                             epochs          = epoch_choice,\n",
    "#                             validation_data= (X_test, Y_test),\n",
    "#                             verbose         = 2,\n",
    "#                             callbacks       = [early_stop])\n",
    "\n",
    "\n",
    "#         loss = model.evaluate((X1_test,X2_test),  Y_test, verbose=0)\n",
    "#         print('loss: ' , loss)\n",
    "\n",
    "#         return {'loss': loss, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "#     # model(*data())\n",
    "#     best_run, best_model = optim.minimize(model=model,\n",
    "#                                               data=data,\n",
    "#                                               algo=tpe.suggest,\n",
    "#                                               max_evals=1000,\n",
    "#                                               trials=Trials(),\n",
    "#                                               notebook_name = 'FusedModel1',\n",
    "#                                               eval_space=True)\n",
    "#     print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c29a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "n_neuron       = 64\n",
    "activation     = 'relu'\n",
    "num_epochs     = 50\n",
    "learning_rate  = 0.0001\n",
    "minibatch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2, build 2nd neural net\n",
    "\n",
    "input2 = Input(shape= (1,))\n",
    "\n",
    "input_concat = Concatenate()([output1, input2 ])\n",
    "dense1_2 = Dense(n_neuron,  activation=activation)\n",
    "# Layer 2\n",
    "dense2_2 = Dense(n_neuron,  activation=activation)\n",
    "# Layer 3\n",
    "dense3_2 = Dense(n_neuron,  activation=activation)\n",
    "#output\n",
    "output_layer_2 = Dense(y_train.shape[1],  activation='linear')\n",
    "#dropout\n",
    "# dropout_layer = Dropout(rate = 0.3)\n",
    "\n",
    "\n",
    "x = dense1_2(input_concat)\n",
    "x = dense2_2(x)\n",
    "x = dense3_2(x)\n",
    "x = dropout_layer(x)\n",
    "output2 = output_layer_2(x)\n",
    "\n",
    "model2 = Model(inputs = [input1, input2], outputs = output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b18bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1.Trainable = False\n",
    "dense2.trainable = False\n",
    "dense3.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd84c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f217d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "history2 = model2.fit([X2_1_train, X2_2_train], y2_train, \n",
    "                    batch_size      = minibatch_size,\n",
    "                    epochs          = num_epochs,\n",
    "                    validation_split= 0.2, \n",
    "                    verbose         = 1,\n",
    "                    callbacks       = [early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history2)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca04d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(in_situ_truth,firstnn_y_test_pre, s = 2)\n",
    "plt.xlabel('Truth')\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3)\n",
    "plt.title('First NN vs In Situ, MSE = {}, $R_p$ = {}'\n",
    "          .format(round(firstnn_loss,3), round(firstnn_pearson_r,3)))\n",
    "plt.show()\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_smap = X2_2_test[:,0]\n",
    "print(y_smap.shape)\n",
    "\n",
    "smap_insitu_r = pearsonr(in_situ_truth, y_smap).statistic\n",
    "smap_insitu_loss = metrics.mean_squared_error(in_situ_truth, y_smap)\n",
    "\n",
    "\n",
    "plt.scatter(y_truth,y_smap, s = 2)\n",
    "plt.xlabel('Truth')\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3)\n",
    "plt.title('SMAP vs In Situ, MSE = {}, $R_p$ = {}'.format(smap_insitu_loss, smap_insitu_r))\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bcc028",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transfer_y_test_pre = model2.predict([X2_1_test, X2_2_test])[:,0]\n",
    "\n",
    "\n",
    "transfer_insitu_r = pearsonr(in_situ_truth, transfer_y_test_pre).statistic\n",
    "transfer_insitu_loss = metrics.mean_squared_error(in_situ_truth, transfer_y_test_pre)\n",
    "\n",
    "plt.scatter(y_truth,y_test_pre, s = 2)\n",
    "plt.xlabel('Truth')\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3)\n",
    "plt.title('Transfer Model vs In Situ Testing, MSE = {}, $R_p$ = {}'.format(transfer_insitu_loss,transfer_insitu_r ))\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c64b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538ad7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
